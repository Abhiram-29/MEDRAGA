{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150e4644-7073-4b37-a0f2-f23bdd57d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_llm(model, prompt, generated_text):\n",
    "    evaluations = {}\n",
    "\n",
    "    # Template for creating evaluation queries\n",
    "    def create_evaluation_query(template, **kwargs):\n",
    "        query = ChatPromptTemplate.from_template(template)\n",
    "        chain = query | model\n",
    "        return float(chain.invoke(kwargs).content)\n",
    "\n",
    "    # Evaluate Relevance\n",
    "    relevance_template = \"Given the context provided by the following prompt: '{prompt}', please evaluate on a scale from 0 to 1, where 1 is highly relevant and 0 is not relevant at all, how relevant is this generated response: '{generated_text}'? Provide a numerical score only.\"\n",
    "    evaluations['relevance'] = create_evaluation_query(relevance_template, prompt=prompt, generated_text=generated_text)\n",
    "\n",
    "    # Evaluate Clarity\n",
    "    clarity_template = \"How clear and easily understandable is this text: '{generated_text}'? Rate its clarity on a scale from 0 to 1, where 1 indicates that the text is very clear and 0 indicates that the text is very unclear. Provide a numerical score only.\"\n",
    "    evaluations['clarity'] = create_evaluation_query(clarity_template, prompt=prompt, generated_text=generated_text)\n",
    "\n",
    "    # Evaluate Coherence\n",
    "    coherence_template = \"On a scale from 0 to 1, with 1 being highly coherent and 0 being not coherent at all, how well do the ideas in this generated text: '{generated_text}' flow together? Consider if the text makes logical sense as a whole. Provide a numerical score only.\"\n",
    "    evaluations['coherence'] = create_evaluation_query(coherence_template, prompt=prompt, generated_text=generated_text)\n",
    "\n",
    "    # Evaluate Detail and Exhaustiveness\n",
    "    detail_template = \"Assessing the detail and exhaustiveness relative to the prompt '{prompt}', how thoroughly does this generated text: '{generated_text}' cover the topic? Rate on a scale from 0 to 1, where 1 is very detailed and exhaustive, and 0 is not detailed at all. Provide a numerical score only.\"\n",
    "    evaluations['details'] = create_evaluation_query(detail_template, prompt=prompt, generated_text=generated_text)\n",
    "\n",
    "    # Evaluate Suitability as an Answer\n",
    "    suitability_template = \"Evaluate the suitability of this generated text: '{generated_text}' as an answer to the original prompt '{prompt}'. On a scale from 0 to 1, where 1 is a perfect answer and 0 is completely unsuitable, provide a numerical score only.\"\n",
    "    evaluations['suitability'] = create_evaluation_query(suitability_template, prompt=prompt, generated_text=generated_text)\n",
    "\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14649a5-6b8e-4ed2-bccd-15a6ff176b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critique(model, prompt, generated_text):\n",
    "    evaluation_weights = {\n",
    "        'relevance': 3,  \n",
    "        'clarity': 1,\n",
    "        'coherence': 0.5,\n",
    "        'details': 1.5,\n",
    "        'suitability': 2  \n",
    "    }\n",
    "    \n",
    "    evaluations = evaluate_with_llm(model, prompt, generated_text)\n",
    "    print(\"Evaluations:\", evaluations)\n",
    "    \n",
    "    # Calculate the weighted sum of the evaluations\n",
    "    weighted_sum = sum(evaluations[aspect] * evaluation_weights.get(aspect, 1) for aspect in evaluations)\n",
    "    \n",
    "    # Calculate the sum of weights for the aspects evaluated\n",
    "    total_weight = sum(evaluation_weights.get(aspect, 1) for aspect in evaluations)\n",
    "    \n",
    "    # Calculate the weighted average of the evaluations\n",
    "    weighted_average = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    return [weighted_average, evaluations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a353be-c085-4e26-bc88-40d8b07ee7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "import os\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "\n",
    "def get_reranked_result(query, top_n=1):\n",
    "  matches = kb.query([Query(text=query)])\n",
    "  docs = extract_documents_texts(matches)\n",
    "  rerank_results = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=docs, top_n=top_n)\n",
    "  texts = []\n",
    "  for rerank_result in rerank_results:\n",
    "      # Accessing the 'text' field in the document attribute of each RerankResult\n",
    "      text = rerank_result.document['text']\n",
    "      texts.append(text)\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8030101-bfa5-4fbc-a10f-4e3c23575618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class QueryDetail:\n",
    "    def __init__(self, query: str):\n",
    "        self.query = query\n",
    "        self.content: List[str] = []\n",
    "        self.critique_score: float = 0.0\n",
    "        self.critique_details: Dict[str, Any] = {}\n",
    "        self.retrieval_needed: bool = False\n",
    "        self.search_needed: bool = False\n",
    "\n",
    "    def add_response(self, model, search) -> None:\n",
    "        \"\"\"Process the query to add response, handle retrieval and critique.\"\"\"\n",
    "        if is_retrieval_needed(model, self.query):\n",
    "            response = \" \".join(get_reranked_result(self.query, top_n=3))\n",
    "            self.retrieval_needed = True\n",
    "        else:\n",
    "            response = \"Some generated answer\"\n",
    "            self.retrieval_needed = False\n",
    "        \n",
    "        self.content.append(response)\n",
    "        \n",
    "        critique_score, critique_details = critique(model, self.query, response)\n",
    "        self.critique_score = critique_score\n",
    "        self.critique_details = critique_details\n",
    "        self.search_needed = critique_score < 0.5\n",
    "\n",
    "        if self.search_needed:\n",
    "            self.search_and_add_results(search)\n",
    "\n",
    "    def search_and_add_results(self, search) -> None:\n",
    "        \"\"\"Perform a search and process the results if critique score is low.\"\"\"\n",
    "        search_result_raw = search.run(self.query)\n",
    "        search_result = str_to_json(search_result_raw) or []\n",
    "        self.content.extend(search_result)\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, model, search, queries: List[str]):\n",
    "        self.model = model\n",
    "        self.search = search\n",
    "        self.queries = [QueryDetail(query) for query in queries]\n",
    "\n",
    "    def process_queries(self) -> List[QueryDetail]:\n",
    "        \"\"\"Process each query in the list.\"\"\"\n",
    "        for query_detail in self.queries:\n",
    "            query_detail.add_response(self.model, self.search)\n",
    "            if query_detail.search_needed:\n",
    "                consolidated_response = consolidate(self.model, query_detail.content)\n",
    "                query_detail.content = [consolidated_response]\n",
    "                critique_score, critique_details = critique(self.model, query_detail.query, consolidated_response)\n",
    "                query_detail.critique_score = critique_score\n",
    "                query_detail.critique_details = critique_details\n",
    "        return self.queries\n",
    "\n",
    "def advanced_rag_query(model, query: str, num_queries: int) -> List[QueryDetail]:\n",
    "    search = SerpAPIWrapper()\n",
    "    initial_queries = generate_queries(model, query, num_queries)[:num_queries]\n",
    "    query_processor = QueryProcessor(model, search, initial_queries)\n",
    "    processed_queries = query_processor.process_queries()\n",
    "    return processed_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ad583-996e-47a9-96c7-b943ee2fd995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
